---
title: "Natural Language Processing Milestone Report"
author: "Scott Arendt"
date: "April 29, 2016"
output: html_document
---

### Natural Language Processing Project

Hello.  Welcome to my milestone report.  So far, I have found this project to be interesting.  When describing the project to my friends, I can point out that their smart phones use natural language processing when they write out a text.  It is interesting to understand some of the steps that would be involved in making my smart phone predict the next word of my text.

It is also intimidating.  I could see myself working on this project full time for the next year and still adding improvements.


### Analysis of the Dataset

We are working with the Corpora provided by Hans Christensen (http://www.corpora.heliohost.org/index.html).  Specifically, I  am using the US English set of files.

Christensen has been collecting Corpora data from the Internet, using blogs, news sites and tweets as his sources.  Here are the summaries of the files that I am using.

Blogs
38,050,950 words
899,289 lines

News 
35,628,125 words
1,010,243 lines

Twitter
31,062,690 words
2,360,149 lines

While all three sources are similar in the number of words, you will see in the following plot that the raw text translates to processed tokens in very different ways.


```{r sampleanalysis, echo=FALSE}
library(ggplot2)
myCorpusData <- data.frame(source=character(0), lineCount=integer(0), nGram= character(0), count= integer(0))
newrow1000 = cbind("Twitter", 1000, "Unigram", 3326)
newrow5000 = cbind("Twitter", 5000, "Unigram", 9917)
newrow10000 = cbind("Twitter", 10000, "Unigram", 15607)
newrow50000 = cbind("Twitter", 50000, "Unigram", 42514)
newrow100000 = cbind("Twitter", "100000", "Unigram", 65520)
myCorpusData <- rbind(myCorpusData,newrow1000,newrow5000,newrow10000,newrow50000,newrow100000)

newrow1000 = cbind("News", 1000, "Unigram", 7897)
newrow5000 = cbind("News", 5000, "Unigram", 22026)
newrow10000 = cbind("News", 10000, "Unigram", 32454)
newrow50000 = cbind("News", 50000, "Unigram", 76906)
newrow100000 = cbind("News", "100000", "Unigram", 96481)
myCorpusData <- rbind(myCorpusData,newrow1000,newrow5000,newrow10000,newrow50000,newrow100000)

newrow1000 = cbind("Blogs", 1000, "Unigram", 7895)
newrow5000 = cbind("Blogs", 5000, "Unigram", 21641)
newrow10000 = cbind("Blogs", 10000, "Unigram", 32012)
newrow50000 = cbind("Blogs", 50000, "Unigram", 79076)
newrow100000 = cbind("Blogs", "100000", "Unigram", 117053)
myCorpusData <- rbind(myCorpusData,newrow1000,newrow5000,newrow10000,newrow50000,newrow100000)

myCorpusData$V4 <- as.numeric(as.character(myCorpusData$V4))


ggplot(data=myCorpusData, aes(x=V2, y=V4, fill=V1)) +
    geom_bar(colour="black", stat="identity", position=position_dodge()) +
   # guides(fill=FALSE) +
    xlab("Sample Size") + ylab("Tokens") +
    ggtitle("Tokens per Sample Size and Source") +
    scale_shape_discrete(name="Experimental") +
    guides(fill=guide_legend(title=NULL))

```

The results above indicate that when the lines are increased, the number of tokens found in each source varies greatly.  It is not surprising that tweets, with a small fixed maximum size, produced far fewer tokens as I processed larger sets of data.

The plot represents unigram tokens.  I found that the differences found in bigrams and trigrams was even more pronounced.

### Prediction Algorithm

I have not finalized the method that I am going to be using for my prediction algorithm.  I am leaning toward a method of weighting the prediction almost solely on frequency counts of the nGrams. 

Now that I am comfortable that my corpus processing algorithm is working in the way that I want, I will be spending the next two days studying methods for prediction, smoothing and the evaluation of the results.  It is my intention to have a working prediction program shortly.

